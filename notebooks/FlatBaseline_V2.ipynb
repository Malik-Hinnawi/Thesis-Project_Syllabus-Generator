{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%pip install PyPDF2 nltk pandas\n",
    "!pip install pdfminer.six\n",
    "!pip install enchant"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "import pdfplumber\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#nltk.download('words') -> do that only once\n",
    "#nltk.download('stopwords') -> do that only once\n",
    "#nltk.download(\"punkt\") -> do that only once"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def find_contents_page(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        start_page = 22\n",
    "        end_page = 28\n",
    "        end_page = min(end_page, len(pdf_reader.pages))\n",
    "        contents_page = \"\"\n",
    "        for page_num in range(start_page, end_page):\n",
    "            page_text = pdf_reader.pages[page_num].extract_text()\n",
    "            lines = page_text.split('\\n')\n",
    "            page_text = ' '.join(lines)\n",
    "            contents_page += page_text\n",
    "        if not contents_page:\n",
    "            return \"Specified pages not found in the PDF\"\n",
    "\n",
    "        return contents_page\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def create_csv(sections):\n",
    "    data = []\n",
    "    for match in sections:\n",
    "        chapter = match[0]\n",
    "        title = match[1]\n",
    "        page = match[2]\n",
    "\n",
    "        data.append({\"Chapter\": chapter, \"Title\": title, \"Page\": page})\n",
    "    df = pd.concat([pd.DataFrame([section]) for section in data], ignore_index=True)\n",
    "\n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def extract_sections(contents_text):\n",
    "    tokens = nltk.word_tokenize(contents_text)\n",
    "    matches = re.findall(r\"(\\d+\\.\\d+)\\s+(.*?)\\s+(\\d+)\", contents_text)\n",
    "\n",
    "    return matches"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Replace 'OS_Main book.pdf' with the path to your PDF file\n",
    "pdf_path = \"Books/OS_Main book.pdf\"\n",
    "contents_page_text = find_contents_page(pdf_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "pdf_reader = PyPDF2.PdfReader(pdf_path)\n",
    "sections = extract_sections(contents_page_text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "sections"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def add_text_to_content_OLD(content_list, pdf_reader): #NOT WORKING\n",
    "    new_content_list = []\n",
    "    for i, (chapter_num, chapter_name, chapter_page) in enumerate(content_list):\n",
    "        content_text = \"\"\n",
    "        for page_num in range(int(chapter_page), int(chapter_page) + 50):\n",
    "            page_text = pdf_reader.pages[page_num].extract_text()\n",
    "            text = page_text[:50].lower()\n",
    "            print(text)\n",
    "            if (\"exercises\" in text) and (\"chapter\" in text):\n",
    "                content_text += \" \" + page_text\n",
    "                break\n",
    "        new_content_list.append((chapter_num, chapter_name, chapter_page, content_text))\n",
    "        \n",
    "    return new_content_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "def add_text_to_content(content_list, pdf_reader): #NEW\n",
    "    new_content_list = []\n",
    "    upcoming_chapter = 2\n",
    "    buffer = \"\"\n",
    "    for page_number in range(29, len(pdf_reader.pages)):\n",
    "        page_content = pdf_reader.pages[page_number].extract_text()\n",
    "        significant_part = page_content[:50].lower()\n",
    "        buffer += \" \" + page_content\n",
    "        if f\"chapter{upcoming_chapter}\" in significant_part or f\"chapter {upcoming_chapter} \" in significant_part:\n",
    "            buffer += content_list[str(upcoming_chapter-1)]\n",
    "            new_content_list.append((upcoming_chapter-1, buffer, page_number))\n",
    "            buffer = \"\"\n",
    "            upcoming_chapter += 1\n",
    "        elif (upcoming_chapter == 22 and \n",
    "                 f\"appendices\" in significant_part):\n",
    "            buffer += content_list['21']\n",
    "            new_content_list.append((upcoming_chapter-1, buffer, page_number))\n",
    "            buffer = \"\"\n",
    "            break\n",
    "    return new_content_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "def pre_process_string(string_X):\n",
    "    processed_string = re.sub(r'\\W', ' ', str(string_X))\n",
    "    processed_string = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_string)\n",
    "    processed_string = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_string)\n",
    "    processed_string = re.sub(r'\\s+', ' ', processed_string, flags=re.I)\n",
    "    processed_string = re.sub(r'^b\\s+', ' ', processed_string)\n",
    "    processed_string = re.sub(r'[A-Z]+[a-z]+\\d+(\\.\\d+)?', \"\", processed_string)\n",
    "    processed_string = re.sub(r'\\d+(\\.\\d+)?', \"\", processed_string)\n",
    "    processed_string = re.sub(r'\\s+', ' ', processed_string, flags=re.I)\n",
    "    processed_string = re.findall(r'[A-Z]+[a-z]*|[a-z]+', processed_string)\n",
    "    processed_string = ' '.join(processed_string)\n",
    "    return processed_string.strip()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "def predict_label(input_string, model):\n",
    "    segment_size = 300\n",
    "    input_segments = split_text(input_string, segment_size)\n",
    "    input_features = vectorizer.transform(input_segments).toarray()\n",
    "    predicted_labels = model.predict(input_features)\n",
    "    return predicted_labels"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "def get_correct_and_wrong_parts(sections):\n",
    "    wrong_contents = []\n",
    "    normal_contents = []\n",
    "    prev_index, prev_index_dec = None, None\n",
    "    prev_sec = None\n",
    "    for section in sections:\n",
    "        current_index, current_index_dec = section[0].split(\".\")\n",
    "        if ((prev_index is not None) and \n",
    "            (int(current_index) == int(prev_index)) and \n",
    "            (int(current_index_dec) != int(prev_index_dec) + 1)):\n",
    "            wrong_contents.append(prev_sec)\n",
    "        else:\n",
    "            if prev_sec is not None:\n",
    "                normal_contents.append(prev_sec)\n",
    "\n",
    "        prev_sec = section\n",
    "        prev_index = current_index\n",
    "        prev_index_dec = current_index_dec\n",
    "    return normal_contents, wrong_contents"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "normal_contents, wrong_contents = get_correct_and_wrong_parts(sections)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "wrong_contents "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "def get_flattened_content(normal_contents):\n",
    "    result = {}\n",
    "    for chapter in normal_contents:\n",
    "        main = chapter[0].split(\".\")[0]\n",
    "        if main not in result.keys():\n",
    "            result[main] = chapter[1]\n",
    "        else:\n",
    "            result[main] += \" \" + chapter[1]\n",
    "    return result"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "flattened_contents = get_flattened_content(normal_contents)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "flattened_contents"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "contents_page_with_text = add_text_to_content(flattened_contents, pdf_reader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "dictionary = {\"chapter\":[], \"words\":[]}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "for section in contents_page_with_text:\n",
    "    dictionary[\"chapter\"].append(section[0])\n",
    "    dictionary[\"words\"].append(re.sub(r'\\b\\w{20,}\\b', '', pre_process_string(section[1]).replace(\" the\", \" \").lower()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "df = pd.DataFrame(dictionary)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "df[\"words_number\"] = df[\"words\"].apply(lambda x: len(x.split()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "def split_text(text, segment_size):\n",
    "    words = text.split()\n",
    "    segments = [words[i:i+segment_size] for i in range(0, len(words), segment_size)]\n",
    "    return [' '.join(segment) for segment in segments]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "def get_augmented_data(df, segment_size):\n",
    "    augmented_data = {\"chapter\": [], \"words\": []}\n",
    "    for index, row in df.iterrows():\n",
    "        chapter = row[\"chapter\"]\n",
    "        words = row[\"words\"]\n",
    "        text_segments = split_text(words, segment_size)\n",
    "        for segment in text_segments:\n",
    "            augmented_data[\"chapter\"].append(chapter)\n",
    "            augmented_data[\"words\"].append(segment)\n",
    "\n",
    "    return pd.DataFrame(augmented_data).sample(frac=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "augmented_df = get_augmented_data(df, 300)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "chapter_counts = augmented_df[\"chapter\"].value_counts()\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(chapter_counts, labels=chapter_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Chapters')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000, \n",
    "                             min_df=0.05, \n",
    "                             max_df=0.85, \n",
    "                             stop_words=stopwords.words('english'))\n",
    "augmented_features = vectorizer.fit_transform(augmented_df[\"words\"]).toarray()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "comparison = {}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "source": [
    "def get_statistics(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\\n\", classification_rep)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    pd.Series(y_pred).value_counts().sort_index().plot(kind='bar', color='darkred')\n",
    "    plt.xlabel('Chapter')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Predicted Chapters')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(augmented_features, augmented_df[\"chapter\"], test_size=0.2, \n",
    "                                                    random_state=0, stratify=augmented_df[\"chapter\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "mlp = MLPClassifier(max_iter=10000, activation=\"tanh\", hidden_layer_sizes=(56))\n",
    "mlp.fit(X_train, y_train)\n",
    "get_statistics(mlp, X_test, y_test)\n",
    "comparison[\"MLP\"] = mlp.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "source": [
    "svc_model = SVC(kernel='rbf', C=1.0, gamma=0.9)\n",
    "svc_model.fit(X_train, y_train)\n",
    "get_statistics(svc_model, X_test, y_test)\n",
    "comparison[\"SVC\"] = svc_model.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "get_statistics(rf_classifier, X_test, y_test)\n",
    "comparison[\"Random Forest\"] = rf_classifier.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "source": [
    "logreg = LogisticRegression(max_iter=1000, multi_class=\"multinomial\")\n",
    "logreg.fit(X_train, y_train)\n",
    "get_statistics(logreg, X_test, y_test)\n",
    "comparison[\"Logistic Regression\"] = logreg.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "nb = MultinomialNB(alpha=.5)\n",
    "nb.fit(X_train, y_train)\n",
    "get_statistics(nb, X_test, y_test)\n",
    "comparison[\"Naive Bayes\"] = nb.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "source": [
    "comparison"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "source": [
    "models = list(comparison.keys())\n",
    "accuracy = list(comparison.values())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models, accuracy, color=['blue', 'orange', 'green', 'red', 'purple'])\n",
    "plt.title('Comparison of ML Models')\n",
    "plt.xlabel('Machine Learning Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)  \n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "source": [
    "input_string = \"\"\"virtual machines and their relationship to contemporary operating\n",
    "systems. Included is a general description of the hardware and software\n",
    "techniques that make virtualization possible. This chapter provides an\n",
    "overview of computer networks and distributed systems, with a focus on\n",
    "the Internet and TCP/IP.\"\"\"\n",
    "predicted_labels = predict_label(input_string, logreg)\n",
    "print(\"Predicted Labels:\", predicted_labels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
