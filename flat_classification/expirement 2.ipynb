{
 "cells": [
  {
   "cell_type": "code",
   "id": "brutal-syndication",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:38:40.041403Z",
     "start_time": "2024-05-22T12:38:37.277769Z"
    }
   },
   "source": [
    "import joblib\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import PyPDF2\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from isodate import parse_duration"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "running-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"models/\"+ i for i in os.listdir(\"models\")]\n",
    "vectorizers= [\"vectorizers/\" + i for i in os.listdir(\"vectorizers\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "earlier-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_INFO = {\"computer_architecture_book\": [\"Computer as components: Wayne Wolf\", None],\n",
    "              \"data_intensive_book\": [\"Designing data intensive applications: O'REILLY\", None],\n",
    "              \"ethic_1\": [\"Ethics in IT: George W.Reynolds\", None],\n",
    "              \"ethic_2\": [\"Ethics for the information age: Michael J. Quinn\", None],\n",
    "              \"os_book\": [\"Operating System Concepts: WILEY\", None],\n",
    "            \"hci\": [\"Human Computer Interaction\", None],\n",
    "            \"JavaScript\": [\"Javascript Cookbook\", None],\n",
    "            \"network_1\": [\"Computer Networking\", None],\n",
    "            \"Robot_OS\": [\"Robot Operating Systems\", None],\n",
    "            \"Robotic_python\": [\"Robotics with Python\", None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "specified-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainProgram():\n",
    "    \n",
    "    _text_data = None\n",
    "    _main_book = None\n",
    "    _models = {}\n",
    "    \n",
    "    def __init__(self, text_data):\n",
    "        self._text_data = text_data\n",
    "        \n",
    "        for i in os.listdir(\"models\"):\n",
    "            tmp = i.split(\"-\")[0]\n",
    "            self._models[tmp] = [joblib.load(\"models/\" + i), \n",
    "                                 joblib.load(\"vectorizers/\" + tmp + \"-vectorizer.joblib\")]\n",
    "    \n",
    "    def get_related_all_chapters(self):\n",
    "        for model in self._models.keys():\n",
    "            probabilities = predicted_probabilities(self._text_data, \n",
    "                                                    self._models[model][0], self._models[model][1])\n",
    "            BOOK_INFO[model][1] = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alternative-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, segment_size):\n",
    "    words = text.split()\n",
    "    segments = [words[i:i+segment_size] for i in range(0, len(words), segment_size)]\n",
    "    return [' '.join(segment) for segment in segments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "finnish-denmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_probabilities(input_string, model, vectorizer):\n",
    "    segment_size = 300\n",
    "    input_segments = split_text(input_string, segment_size)\n",
    "    input_features = vectorizer.transform(input_segments).toarray()\n",
    "    predicted_prob = model.predict_proba(input_features)\n",
    "    result = zip(predicted_prob[0], model.classes_)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "id": "exterior-keeping",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:08.588192Z",
     "start_time": "2024-05-22T12:46:08.574475Z"
    }
   },
   "source": [
    "def find_contents_page_CAB(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            tmp = None\n",
    "            if \"APPENDIX\" in line[:9]:\n",
    "                break\n",
    "            if not is_content_started:\n",
    "                if \"CHAPTER\" in line:\n",
    "                    is_content_started = True\n",
    "                    number = 1\n",
    "                    match = re.findall(r\"CHAPTER \\d\\s+\", line)\n",
    "                    chapter_name = line.split(match[0])[1]\n",
    "                    result[number] = chapter_name.strip().rstrip().lower()\n",
    "            else:\n",
    "                if \"CHAPTER\" in line:\n",
    "                    number += 1\n",
    "                    match = re.findall(r\"CHAPTER \\d\\s+\", line)\n",
    "                    chapter_name = line.split(match[0])[1]\n",
    "                    result[number] = chapter_name.strip().rstrip().lower()\n",
    "                    \n",
    "                else:\n",
    "                    line = line.replace(\"-\", \"\").replace(\",\", \n",
    "                                                         \"\").replace(\":\", \n",
    "                                                                     \"\").replace(\";\", \n",
    "                                                                                 \"\").replace(\"/\", \n",
    "                                                                                             \"\").replace(\"&\", \" \")\n",
    "                    pattern = re.compile(r\"\\d[.]\\d\\s+([a-z\\s]+)\", re.IGNORECASE)\n",
    "                    match = pattern.findall(line)\n",
    "                    if len(match) != 0:\n",
    "                        result[number] += \", \" + match[0].rstrip().strip().lower()\n",
    "                \n",
    "                \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:39:07.269313Z",
     "start_time": "2024-05-22T12:39:06.875397Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7fe028231f2c6ca5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'find_contents_page_CAB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mfind_contents_page_CAB\u001B[49m()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'find_contents_page_CAB' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "exposed-trace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:11.173971Z",
     "start_time": "2024-05-22T12:46:11.159084Z"
    }
   },
   "source": [
    "def find_contents_page_DIB(pdf_reader, start_page, end_page):\n",
    "    result = []\n",
    "    output = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    number_two = None\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            line = line.replace(\".\", \" \")\n",
    "            matches = re.findall(r\"\\s\\s\\s+\", line)\n",
    "            if len(matches) != 0 and line.split(matches[0])[1].isnumeric():\n",
    "                tmp = line.split(matches[0])\n",
    "                matched_numbers = re.findall(r\"\\d+\\s\", tmp[0])\n",
    "                if len(matched_numbers) != 0:\n",
    "                    number = int(matched_numbers[0].rstrip())\n",
    "                    number_two = 0\n",
    "                    line_splitted = (f\"{number}.{number_two}\", tmp[0].replace(matched_numbers[0], \"\").lower(), tmp[1])\n",
    "                else:\n",
    "                    number_two += 1\n",
    "                    line_splitted = (f\"{number}.{number_two}\", tmp[0].lower(), tmp[1])\n",
    "                result.append(line_splitted)\n",
    "    for i in result:\n",
    "        unit = i[0].split(\".\")[0]\n",
    "        if unit not in output.keys():\n",
    "            output[unit] = i[1]\n",
    "        else: output[unit] += \", \" + i[1]\n",
    "    return output"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "american-utility",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:13.696815Z",
     "start_time": "2024-05-22T12:46:13.688526Z"
    }
   },
   "source": [
    "def find_contents_page_E1(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            tmp = None\n",
    "            if \"Glossary\" in line[:9]:\n",
    "                break\n",
    "            if not is_content_started:\n",
    "                if \"Chapter\" in line[:9]:\n",
    "                    is_content_started = True\n",
    "                    number = 1\n",
    "                    match = re.findall(r\"Chapter \\d+\\s+\", line)\n",
    "                    chapter_name = line.split(match[0])[1]\n",
    "                    result[number] = chapter_name.strip().rstrip().lower()\n",
    "            else:\n",
    "                if \"Chapter\" in line[:9]:\n",
    "                    number += 1\n",
    "                    match = re.findall(r\"Chapter \\d+\\s+\", line)\n",
    "                    chapter_name = line.split(match[0])[1]\n",
    "                    result[number] = chapter_name.strip().rstrip().lower()\n",
    "                    \n",
    "                else:\n",
    "                    line = line.replace(\"-\", \"\").replace(\",\", \n",
    "                                                         \"\").replace(\":\", \n",
    "                                                                     \"\").replace(\";\", \n",
    "                                                                                 \"\").replace(\"/\", \n",
    "                                                                                             \"\").replace(\"&\", \" \")\n",
    "                    match = re.findall(r\"\\s+\\d+\", line)\n",
    "                    \n",
    "                    if len(match) != 0:\n",
    "                        result[number] += \", \" + line.split(match[0])[0].rstrip().strip().lower()\n",
    "                \n",
    "                \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "frozen-democracy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:58:55.708538Z",
     "start_time": "2024-05-22T12:58:55.686644Z"
    }
   },
   "source": [
    "def find_contents_page_E2(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            line = line.replace(\"-\", \" \").replace(\",\", \n",
    "                                                \"\").replace(\":\", \n",
    "                                                        \"\").replace(\";\", \n",
    "                                                                \"\").replace(\"/\", \n",
    "                                                                        \" \").replace(\"&\", \n",
    "                                                                                \" \").replace('\"',\n",
    "                                                                                        \"\").replace(\"'\",\n",
    "                                                                                                   \"\").replace(\"(\",\n",
    "                                                                                                               \"\").replace(\")\", \"\")\n",
    "            match = re.findall(r\"(\\d+)[.]*\\d*[.]*\\d*\\s+[a-zA-Z]*\", line)\n",
    "            line_mod = line.replace(\".\", \"\").replace(\"’\", \" \").replace(\"?\", \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "            match_2 = re.findall(r\"\\d+[.]*\\d*[.]*\\d*([a-zA-Z|\\s]+)\\d+\", line_mod)\n",
    "            \n",
    "            \n",
    "            if len(match) != 0:\n",
    "                if match[0] in result.keys():\n",
    "                    result[match[0]] += \", \" + match_2[0].rstrip().strip().lower()\n",
    "                else:\n",
    "                    result[match[0]] = match_2[0].rstrip().strip().lower()\n",
    "                 \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "headed-nomination",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:40:57.569541Z",
     "start_time": "2024-05-22T12:40:57.271518Z"
    }
   },
   "source": [
    "pdf = PyPDF2.PdfReader(\"Books/data_intensive.pdf\")\n",
    "find_contents_page_DIB(pdf, 6, 12)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'reliable, scalable and maintainable applications, thinking about data systems, reliability, hardware faults, software errors, human errors, how important is reliability?, scalability, describing load, describing performance, approaches for coping with load, maintainability, operability: making life easy for operations, simplicity: managing complexity, evolvability: making change easy, summary',\n",
       " '2': 'data models and query languages, relational model vs  document model, the birth of nosql, the object-relational mismatch, many-to-one and many-to-many relationships, are document databases repeating history?, relational vs  document databases today, query languages for data, declarative queries on the web, mapreduce querying, graph-like data models, property graphs, the cypher query language, graph queries in sql, triple-stores and sparql, the foundation: datalog, summary',\n",
       " '3': 'storage and retrieval, data structures that power your database, hash indexes, sstables and lsm-trees, b-trees, other indexing structures, keeping everything in memory, transaction processing or analytics?, data warehousing, stars and snowflakes: schemas for analytics, column-oriented storage, column compression, sort order in column storage, writing to column-oriented storage, aggregation: data cubes and materialized views, summary',\n",
       " '4': 'encoding and evolution, formats for encoding data, language-specific formats, json, xml and binary variants, thrift and protocol buffers, avro, the merits of schemas, modes of data flow, data flow through databases, data flow through services: rest and rpc, message passing data flow, summary',\n",
       " '5': 'replication, leaders and followers, synchronous vs  asynchronous replication, setting up new followers, handling node outages, implementation of replication logs, problems with replication lag, reading your own writes, monotonic reads, consistent prefix reads, solutions for replication lag, multi-leader replication, use cases for multi-leader replication, handling write conflicts, multi-leader replication topologies, leaderless replication, writing to the database when a node is down, limitations of quorum consistency, sloppy quorums and hinted handoff, detecting concurrent writes, summary',\n",
       " '6': 'partitioning, partitioning and replication, partitioning of key-value data, partitioning by key range, partitioning by hash of key, skewed workloads and relieving hot spots, partitioning and secondary indexes, partitioning secondary indexes by document, partitioning secondary indexes by term, rebalancing partitions, strategies for rebalancing, operations: automatic or manual rebalancing, request routing, parallel query execution, summary',\n",
       " '7': 'transactions, the slippery concept of a transaction, the meaning of acid, single-object and multi-object operations, weak isolation levels, read committed, snapshot isolation and repeatable read, preventing lost updates, preventing write skew and phantoms, serializability, actual serial execution, two-phase locking (2pl), serializable snapshot isolation (ssi), summary',\n",
       " '8': 'the trouble with distributed systems, faults and partial failures, cloud computing and supercomputing, unreliable networks, network faults in practice, detecting faults, timeouts and unbounded delays, synchronous vs  asynchronous networks, unreliable clocks, monotonic vs  time-of-day clocks, clock synchronization and accuracy, relying on synchronized clocks, process pauses, knowledge, truth and lies, the truth is defined by the majority, byzantine faults, system model and reality, summary',\n",
       " '9': 'consistency and consensus, consistency guarantees, linearizability, what makes a system linearizable?, relying on linearizability, implementing linearizable systems, the cost of linearizability, ordering guarantees, ordering and causality, sequence number ordering, total order broadcast, distributed transactions and consensus, atomic commit and two-phase commit (2pc), distributed transactions in practice, fault-tolerant consensus, membership and coordination services, summary',\n",
       " '10': ' batch processing, batch processing with unix tools, simple log analysis, the unix philosophy, mapreduce and distributed filesystems, mapreduce job execution, reduce-side joins and grouping, map-side joins, the output of batch workflows, comparing mapreduce to distributed databases, beyond mapreduce, materialization of intermediate state, graphs and iterative processing, high-level apis and languages, summary',\n",
       " '11': ' stream processing, transmitting event streams, messaging systems, partitioned logs, databases and streams, keeping systems in sync, change data capture, event sourcing, state, streams, and immutability, processing streams, uses of stream processing, reasoning about time, stream joins, fault tolerance, summary'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "early-pillow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:20.111791Z",
     "start_time": "2024-05-22T12:46:20.094907Z"
    }
   },
   "source": [
    "def find_contents_page_OS(pdf_reader):\n",
    "    result = {}\n",
    "    start_page = 22\n",
    "    end_page = 28\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    contents_page = \"\"\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        lines = page_text.split('\\n')\n",
    "        page_text = ' '.join(lines)\n",
    "        contents_page += page_text\n",
    "    if not contents_page:\n",
    "        return \"Specified pages not found in the PDF\"\n",
    "\n",
    "    matches = re.findall(r\"(\\d+\\.\\d+)\\s+(.*?)\\s+(\\d+)\", contents_page)\n",
    "    for i in matches:\n",
    "        unit = i[0].split(\".\")[0]\n",
    "        if unit not in result.keys():\n",
    "            result[unit] = i[1]\n",
    "        else: result[unit] += \", \" + i[1]\n",
    "    return result\n",
    "def find_contents_page_HCI(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            line = line.replace(\"-\", \" \").replace(\",\", \n",
    "                                                \"\").replace(\":\", \n",
    "                                                        \"\").replace(\";\", \n",
    "                                                                \"\").replace(\"/\", \n",
    "                                                                        \" \").replace(\"&\", \n",
    "                                                                                \" \").replace('\"',\n",
    "                                                                                        \"\").replace(\"'\",\n",
    "                                                                                                   \"\").replace(\"(\",\n",
    "                                                                                                               \"\").replace(\")\", \"\")\n",
    "            match = re.findall(r\"(\\d+)[.]*\\d*[.]*\\d*\\s+[a-zA-Z]*\", line)\n",
    "            line_mod = line.replace(\".\", \"\").replace(\"’\", \n",
    "                                                     \" \").replace(\"?\", \n",
    "                                                                  \"\").replace(\"“\", \n",
    "                                                                              \"\").replace(\"”\", \n",
    "                                                                                          \"\").replace(\"#\", \n",
    "                                                                                                      \"\").replace(\"!\", \"\")\n",
    "            match_2 = re.findall(r\"\\d+[.]*\\d*[.]*\\d*([a-zA-Z|\\s]+)\", line_mod)\n",
    "            \n",
    "            \n",
    "            if len(match) != 0:\n",
    "                if match[0] in result.keys():\n",
    "                    result[match[0]] += \", \" + match_2[0].rstrip().strip().lower()\n",
    "                else:\n",
    "                    result[match[0]] = match_2[0].rstrip().strip().lower()\n",
    "                 \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "balanced-florida",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:25.152733Z",
     "start_time": "2024-05-22T12:46:25.144566Z"
    }
   },
   "source": [
    "def find_contents_page_JS(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            line = line.replace(\"-\", \" \").replace(\",\", \n",
    "                                                \"\").replace(\":\", \n",
    "                                                        \"\").replace(\";\", \n",
    "                                                                \"\").replace(\"/\", \n",
    "                                                                        \" \").replace(\"&\", \n",
    "                                                                                \" \").replace('\"',\n",
    "                                                                                        \"\").replace(\"'\",\n",
    "                                                                                                   \"\").replace(\"(\",\n",
    "                                                                                                               \"\").replace(\")\", \"\")\n",
    "            match = re.findall(r\"Chapter\\s*(\\d+)\", line)\n",
    "            line_mod = line.replace(\".\", \"\").replace(\"’\", \n",
    "                                                     \" \").replace(\"?\", \n",
    "                                                                  \"\").replace(\"“\", \n",
    "                                                                              \"\").replace(\"”\", \n",
    "                                                                                          \"\").replace(\"#\", \n",
    "                                                                                                      \"\").replace(\"!\", \"\")\n",
    "            match_2 = re.findall(r\"\\d+\\s+([a-zA-Z|\\s]+)\", line_mod)\n",
    "            match_3 = re.findall(r\"\\s*([a-zA-Z|\\s]+)\", line_mod)\n",
    "\n",
    "            if len(match) != 0:\n",
    "                number = match[0]\n",
    "                result[number] = match_2[0].rstrip().strip().lower()\n",
    "            else:\n",
    "                if len(match_2) == 0 and len(match) == 0 and len(match_3) != 0 and number is not None:\n",
    "                    result[number] +=  \", \" + match_3[0].rstrip().strip().lower()\n",
    "                 \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "ready-philip",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:27.811116Z",
     "start_time": "2024-05-22T12:46:27.765196Z"
    }
   },
   "source": [
    "def find_contents_page_N1(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    result_2 = []\n",
    "    \n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            line = line.replace(\"-\", \" \").replace(\",\", \n",
    "                                                \"\").replace(\":\", \n",
    "                                                        \"\").replace(\";\", \n",
    "                                                                \"\").replace(\"/\", \n",
    "                                                                        \" \").replace(\"&\", \n",
    "                                                                                \" \").replace('\"',\n",
    "                                                                                        \"\").replace(\"'\",\n",
    "                                                                                                   \"\").replace(\"(\",\n",
    "                                                                                                               \"\").replace(\")\", \"\")\n",
    "            \n",
    "            match = re.findall(r\"Chapter\\s+(\\d+)\\s+[a-zA-Z|\\s]+\", line)\n",
    "            match_subsections = re.findall(r\"(\\d+)[.]\\d+[.]*\\d*\\s+[a-zA-Z|\\s]+\", line)\n",
    "            \n",
    "            line_mod = line.replace(\"’\", \" \").replace(\"?\", \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "            \n",
    "            result_2.append(line_mod.lower())\n",
    "            \n",
    "            line_mod = line_mod.replace(\".\", \" \")\n",
    "            \n",
    "            match_2 = re.findall(r\"Chapter\\s+\\d+\\s+([a-zA-Z|\\s]+)\", line_mod)\n",
    "            match_subsections_2 = re.findall(r\"\\d+\\s\\d+\\s+\\d*\\s*([a-zA-Z|\\s]+)\", line_mod)\n",
    "            if len(match) != 0:\n",
    "                result[match[0]] = match_2[0].rstrip().strip().lower()\n",
    "                number = match[0]\n",
    "            elif len(match_subsections_2) != 0 and number is not None:\n",
    "                result[number] += \", \" + match_subsections_2[0].rstrip().strip().lower()\n",
    "                 \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "informational-projector",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:30.029171Z",
     "start_time": "2024-05-22T12:46:30.021290Z"
    }
   },
   "source": [
    "def find_contents_page_RO(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            line = line.replace(\"-\", \" \").replace(\",\", \n",
    "                                                \"\").replace(\":\", \n",
    "                                                        \"\").replace(\";\", \n",
    "                                                                \"\").replace(\"/\", \n",
    "                                                                        \" \").replace(\"&\", \n",
    "                                                                                \" \").replace('\"',\n",
    "                                                                                        \"\").replace(\"'\",\n",
    "                                                                                                   \"\").replace(\"(\",\n",
    "                                                                                                               \"\").replace(\")\", \"\")\n",
    "            match = re.findall(r\"Chapter\\s+(\\d+)\\s+[a-zA-Z|\\s]+\", line)\n",
    "            line_mod = line.replace(\".\", \" \").replace(\"’\", \" \").replace(\"?\", \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "            match_2 = re.findall(r\"Chapter\\s+\\d+\\s+([a-zA-Z|\\s]+)\", line_mod)\n",
    "            match_subsections = re.findall(r\"\\s*([a-zA-Z|\\s]+)\", line)\n",
    "            if len(match) != 0:\n",
    "                result[match[0]] = match_2[0].rstrip().strip().lower()\n",
    "                number = match[0]\n",
    "            elif len(match_subsections) != 0 and number is not None:\n",
    "                result[number] += \", \" + match_subsections[0].rstrip().strip().lower()\n",
    "                 \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "crazy-audience",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:46:32.650017Z",
     "start_time": "2024-05-22T12:46:32.631668Z"
    }
   },
   "source": [
    "def find_contents_page_RP(pdf_reader, start_page, end_page):\n",
    "    result = {}\n",
    "    end_page = min(end_page, len(pdf_reader.pages))\n",
    "    number = None\n",
    "    is_content_started = False\n",
    "\n",
    "    for page_num in range(start_page, end_page):\n",
    "        page_text = pdf_reader.pages[page_num].extract_text()\n",
    "        for line in page_text.split(\"\\n\"):\n",
    "            line = line.replace(\"-\", \" \").replace(\",\", \n",
    "                                                \"\").replace(\":\", \n",
    "                                                        \"\").replace(\";\", \n",
    "                                                                \"\").replace(\"/\", \n",
    "                                                                        \" \").replace(\"&\", \n",
    "                                                                                \" \").replace('\"',\n",
    "                                                                                        \"\").replace(\"'\",\n",
    "                                                                                                   \"\").replace(\"(\",\n",
    "                                                                                                               \"\").replace(\")\", \"\")\n",
    "            match = re.findall(r\"Chapter\\s+(\\d+)\\s+[a-zA-Z|\\s]+\", line)\n",
    "            line_mod = line.replace(\".\", \" \").replace(\"’\", \" \").replace(\"?\", \"\").replace(\"“\", \"\").replace(\"”\", \"\")\n",
    "            match_2 = re.findall(r\"Chapter\\s+\\d+\\s+([a-zA-Z|\\s]+)\", line_mod)\n",
    "            match_subsections = re.findall(r\"\\s*([a-zA-Z|\\s]+)\", line)\n",
    "            if len(match) != 0:\n",
    "                result[match[0]] = match_2[0].rstrip().strip().lower()\n",
    "                number = match[0]\n",
    "            elif len(match_subsections) != 0 and number is not None:\n",
    "                result[number] += \", \" + match_subsections[0].rstrip().strip().lower()\n",
    "                 \n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sexual-sherman",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit(book, chapter):\n",
    "    result = \"\"\n",
    "    flattened_contents = None\n",
    "    \n",
    "    if book == \"computer_architecture_book\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/computer_architecture.pdf\")\n",
    "        flattened_contents = find_contents_page_CAB(pdf, 9, 17)\n",
    "    elif book == \"data_intensive_book\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/data_intensive.pdf\")\n",
    "        flattened_contents = find_contents_page_DIB(pdf, 6, 12)\n",
    "    elif book == \"ethic_1\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/ethics_1.pdf\")\n",
    "        flattened_contents = find_contents_page_E1(pdf, 8, 14)\n",
    "    elif book == \"ethic_2\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/ethics_2.pdf\")\n",
    "        flattened_contents = find_contents_page_E2(pdf, 6, 28)\n",
    "    elif book == \"os_book\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/os.pdf\")\n",
    "        flattened_contents = find_contents_page_OS(pdf)\n",
    "    elif book == \"hci\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/HCI.pdf\")\n",
    "        flattened_contents = find_contents_page_HCI(pdf, 5, 10)\n",
    "    elif book == \"JavaScript\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/JavaScript.pdf\")\n",
    "        flattened_contents = find_contents_page_JS(pdf, 4, 14)\n",
    "    elif book == \"network_1\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/network_1.pdf\")\n",
    "        flattened_contents = find_contents_page_N1(pdf, 4, 11)\n",
    "    elif book == \"Robot_OS\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/Robot_OS.pdf\")\n",
    "        flattened_contents = find_contents_page_RO(pdf, 4, 9)\n",
    "    elif book == \"Robotic_python\":\n",
    "        pdf = PyPDF2.PdfReader(\"Books/Robotics_python.pdf\")\n",
    "        flattened_contents = find_contents_page_RP(pdf, 4, 9)\n",
    "    \n",
    "    normalized_dict = {str(key): value for key, value in flattened_contents.items()}\n",
    "    return normalized_dict[str(chapter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "color-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hours(x):\n",
    "    output = 1\n",
    "    if x >= 0.5:\n",
    "        output = (48 * x**2) + 2\n",
    "    elif x >= 0.4:\n",
    "        output = (28 * x**2) + ((2*x)**4)\n",
    "    elif x >= 0.3:\n",
    "        output = (12 * x**2) + ((1.5*x)**3)\n",
    "    elif x >= 0.2:\n",
    "        output = (6 * x**2)\n",
    "    \n",
    "    return math.ceil(max(output, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "macro-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_words(content):\n",
    "    WORDS_TO_EXCLUDE = [\"overview\", \"summary\", \"intro\", \"introduction\", \n",
    "                        \"abstraction\", \"background\", \"start\"]\n",
    "    result = \"\"\n",
    "    number = 0\n",
    "    if \",\" in content:\n",
    "        tmp = content.split(\",\")\n",
    "        for word in tmp:\n",
    "            word = word.strip().rstrip().lower()\n",
    "            if not (word in WORDS_TO_EXCLUDE):\n",
    "                result += word + \" \"\n",
    "                number += 1\n",
    "                if number == 2:\n",
    "                    return result + \"in computer science\"\n",
    "    else:\n",
    "        tmp = content.split(\" \")\n",
    "        for word in tmp:\n",
    "            word = word.strip().rstrip().lower()\n",
    "            result += word + \" \"\n",
    "            number += 1\n",
    "            if number >= 8:\n",
    "                return result + \"in computer science\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "black-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistics_per_book():\n",
    "    result = []\n",
    "    for i in BOOK_INFO.keys():\n",
    "        book = BOOK_INFO[i]\n",
    "        for j in list(book[1]):\n",
    "            if float(j[0]) > 0.30:\n",
    "                unit_content = get_unit(i, j[1])\n",
    "                result.append([book[0], j[0], j[1],\n",
    "                               unit_content, get_hours(float(j[0]))])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "loaded-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sortedBy_importance(result):\n",
    "    output = []\n",
    "    tmp = [i[1] for i in result]\n",
    "    tmp.sort(reverse=True)\n",
    "    for t in tmp:\n",
    "        for j in result:\n",
    "            if t == j[1]:\n",
    "                output.append(j)\n",
    "    if len(output) > 0:\n",
    "        output[0].append(youtube_videos(get_key_words(output[0][3])))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "worthy-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(sorted_results):\n",
    "    print(\"GENERATED SYLLABUS:\")\n",
    "    print(\"--\"*45)\n",
    "    n = 1\n",
    "    for i in sorted_results:\n",
    "        output = f\"\"\"[{i[4]} HOURS] FROM '{i[0]}', you should look chapter {i[2]}.\\n\n",
    "        \\tChapter {i[2]} Main Subjects:\\n\\t\\t\\t{i[3]}\"\"\"\n",
    "        \n",
    "        if n == 1 and len(i) >= 6:\n",
    "            output += f\"\\n\\tPlease watch {i[5][0]} from the link below:\\n\\t\\t\\t {i[5][1]}\"\n",
    "        output += \"\\n\\n\\n\"\n",
    "        \n",
    "        print(output)\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "korean-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_youtube_videos(api_key, query, max_results=5):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Call the search.list method to retrieve results matching the specified query term.\n",
    "    search_response = youtube.search().list(\n",
    "        q=query,\n",
    "        part='id,snippet',\n",
    "        maxResults=max_results\n",
    "    ).execute()\n",
    "\n",
    "    videos = []\n",
    "\n",
    "    # Add each result to the list\n",
    "    for search_result in search_response.get('items', []):\n",
    "        if search_result['id']['kind'] == 'youtube#video':\n",
    "            videos.append({\n",
    "                'title': search_result['snippet']['title'],\n",
    "                'video_id': search_result['id']['videoId'],\n",
    "                'url': f\"https://www.youtube.com/watch?v={search_result['id']['videoId']}\"\n",
    "            })\n",
    "\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "sudden-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_duration(api_key, video_id):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    # Call the videos.list method to retrieve information about the specified video.\n",
    "    videos_response = youtube.videos().list(\n",
    "        part='contentDetails',\n",
    "        id=video_id\n",
    "    ).execute()\n",
    "\n",
    "    # Extract duration from the response\n",
    "    duration = videos_response['items'][0]['contentDetails']['duration']\n",
    "\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fluid-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_popular_videos(api_key, query):\n",
    "    videos = search_youtube_videos(api_key, query)\n",
    "\n",
    "    if not videos:\n",
    "        return None\n",
    "\n",
    "    for video in videos:\n",
    "        duration = parse_duration(get_video_duration(api_key, video['video_id']))\n",
    "        if duration.total_seconds() > 300:  # 20 minutes in seconds\n",
    "            return video\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "oriental-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtube_videos(query):\n",
    "    output = None\n",
    "    api_key = 'AIzaSyD9PTkbmrEzzdr7IcUmjbJKZqgdHldz09g' # benim API key'im ama bu restricted access sağlıyor\n",
    "    most_popular_video = get_most_popular_videos(api_key, query)\n",
    "    if most_popular_video:\n",
    "        output = [most_popular_video['title'], most_popular_video['url']]\n",
    "    time.sleep(5)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "sound-maryland",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = \"\"\"standardized system utilized in model railroading \n",
    "to manage locomotives and accessories via a digital signal transmitted through\n",
    "the tracks. Unlike traditional analog systems where locomotives receive power directly\n",
    "from the tracks, it enables precise control of multiple trains independently \n",
    "on the same track without the need for separate wiring or electrical blocks. \n",
    "Each locomotive is equipped with a decoder that receives commands from a central \n",
    "controller or throttle, allowing operators to control speed, direction, lighting,\n",
    "and sound functions. This systems offer enhanced realism and flexibility, enabling model\n",
    "railroaders to replicate real-world train operations more accurately and create \n",
    "immersive layouts with intricate control over their trains and accessories.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "furnished-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string_2 = \"\"\"\n",
    "In computing, threads enable programs to execute multiple tasks simultaneously. \n",
    "They divide the workload into smaller chunks, allowing for efficient resource allocation and parallel execution. \n",
    "This enhances performance and responsiveness, akin to a juggler effortlessly managing multiple objects at once.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "inappropriate-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string_3 = \"\"\"When considering database models, the document type model offers a higher degree of flexibility compared to relational models. Document type models excel particularly in handling tree-based data structures, where hierarchical relationships are prevalent. In this model, data is stored in a document format, such as JSON or XML, allowing for nested and varied structures within each document. This flexibility accommodates evolving data schemas and unstructured data well, making it suitable for applications with evolving data requirements or diverse data formats.\n",
    "Conversely, relational database models are well-suited for managing many-to-many relationships between entities. These models organize data into tables with rows and columns, enforcing a structured schema defined by the relational schema. This structure facilitates efficient querying and data retrieval, especially when dealing with complex relationships between entities. Relational databases excel in maintaining data integrity through normalization techniques, ensuring consistency and accuracy in data storage and retrieval operations.\n",
    "In summary, while document type models prioritize flexibility and adaptability, making them ideal for managing tree-based data, relational database models excel in handling many-to-many relationships and ensuring data integrity within structured environments. The choice between these models depends on the specific requirements and characteristics of the data and the application context.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "obvious-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "main = MainProgram(input_string_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "convinced-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "main.get_related_all_chapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "insured-hardware",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_sortedBy_importance(get_statistics_per_book())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "wrapped-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATED SYLLABUS:\n",
      "------------------------------------------------------------------------------------------\n",
      "[21 HOURS] FROM 'Operating System Concepts: WILEY', you should look chapter 4.\n",
      "\n",
      "        \tChapter 4 Main Subjects:\n",
      "\t\t\tOverview, Multicore Programming, Multithreading Models, ThreadLibraries, Implicit Threading, Operating-System Examples, Summary\n",
      "\tPlease watch Multithreading Models &amp; Hyperthreading from the link below:\n",
      "\t\t\t https://www.youtube.com/watch?v=HW2Wcx-ktsc\n",
      "\n",
      "\n",
      "\n",
      "[7 HOURS] FROM 'Computer Networking', you should look chapter 2.\n",
      "\n",
      "        \tChapter 2 Main Subjects:\n",
      "\t\t\tapplication layer, principles of network applications, network application architectures, processes communicating, transport services available to applications, transport services provided by the internet, application layer protocols, network applications covered in this book, the web and http, overview of http, non persistent and persistent connections, http message format, user server interaction cookies, web caching, http, electronic mail in the internet, smtp, mail message formats, mail access protocols, dns, services provided by dns, overview of how dns works, dns records and messages, peer to peer file distribution, video streaming and content distribution networks, internet video, http streaming and dash, content distribution networks, case studies netflix and youtube, socket programming creating network applications, socket programming with udp, socket programming with tcp, summary, \n",
      "\n",
      "\n",
      "\n",
      "[7 HOURS] FROM 'Robotics with Python', you should look chapter 2.\n",
      "\n",
      "        \tChapter 2 Main Subjects:\n",
      "\t\t\tan intr oduction to raspberry pi, downloading and  installing raspbian, raspbian with  opencv, the, the, connecting raspberry pi, configuring your pi, about the t echnical reviewer, introduction, viusing raspi config, users, connecting to  a wireless network, going headless, remote access, summary\n",
      "\n",
      "\n",
      "\n",
      "[3 HOURS] FROM 'Ethics for the information age: Michael J. Quinn', you should look chapter 1.\n",
      "\n",
      "        \tChapter 1 Main Subjects:\n",
      "\t\t\tcatalysts for change, introduction, milestones in computing, aids to manual calculating, mechanical calculators, cash register, punched card tabulation, precursors of commercial computers, first commercial computers, programming languages and time sharing, transistor and integrated circuit, ibm system, microprocessor, personal computer, milestones in networking, electricity and electromagnetism, telegraph, telephone, typewriter and teletype, radio, television, remote computing, arpanet, email, internet, nsfnet, broadband, wireless networks, cloud computing, milestones in information storage and retrieval, greek alphabet, codex and paper, gutenberg s printing press, newspapers, hypertext, graphical user interface, single computer hypertext systems, networked hypertext world wide web, search engines, cloud storage, contemporary information technology issues, introduction, arguments and propositions, conditional statements, backing\n",
      "\n",
      "\n",
      "\n",
      "[2 HOURS] FROM 'Robot Operating Systems', you should look chapter 4.\n",
      "\n",
      "        \tChapter 4 Main Subjects:\n",
      "\t\t\tkick starting robot pr ogramming using ros, what is robot programming, why robot programming is different, getting started with  ros, the ros equation, robot prog ramming before and  after ros, the history of  ros, before and  after ros, why use ros, installing ros, robots and  sensors supporting ros, popular ros computing platforms, ros architecture and  concepts, the ros file system, ros computation concepts, the ros community, ros command tools, ros demo hello world example, ros demo turtlesim, ros gui tools rviz and  rqt, summary\n",
      "\n",
      "\n",
      "\n",
      "[2 HOURS] FROM 'Computer as components: Wayne Wolf', you should look chapter 5.\n",
      "\n",
      "        \tChapter 5 Main Subjects:\n",
      "\t\t\tprogram design and analysis 209, components for embedded programs, state machines, streamoriented programming and circular, queues, models of programs, data flow graphs, controldata flow graphs, assembly  linking and loading, assemblers, linking, basic compilation techniques, statement translation, procedures, data structures, program optimization, expression simpli, dead code elimination, procedure inlining, loop transformations, register allocation, scheduling, instruction selection, understanding and using your compiler, interpreters and jit compilers, programlevel performance analysis, elements of program performance, measurementdriven performance analysis, software performance optimization, loop optimizations, performance optimization strategies, programlevel energy and power analysis, analysis and optimization of program size, clearbox testing, blackbox testing, evaluating function tests, theory of operation and requirements, speci, system architecture, component design and testing, system integration and testing\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_output(results)"
   ]
  },
  {
   "cell_type": "code",
   "id": "warming-catholic",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:45:14.200515Z",
     "start_time": "2024-05-22T12:45:14.191425Z"
    }
   },
   "source": [
    "import os\n",
    "import csv"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "naked-greensboro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:50:43.140332Z",
     "start_time": "2024-05-22T12:50:43.124619Z"
    }
   },
   "source": [
    "def save_to_csv(result, book_name, start, end):\n",
    "        # Ensure the \"results/\" directory exists\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "        # Define the CSV file path\n",
    "        csv_file_path = f\"results/{book_name}.csv\"\n",
    "\n",
    "        # Write the result dictionary to a CSV file\n",
    "        with open(csv_file_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Key\", \"Value\"])\n",
    "            for key, value in result.items():\n",
    "                writer.writerow([key, value])\n",
    "\n",
    "        print(f\"CSV file created at: {csv_file_path}\")"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:59:15.066908Z",
     "start_time": "2024-05-22T12:59:07.221068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/computer_architecture.pdf\")\n",
    "flattened_contents = find_contents_page_CAB(pdf, 9, 17)\n",
    "save_to_csv(flattened_contents, \"computer_architecture\", 9, 17)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/data_intensive.pdf\")\n",
    "flattened_contents = find_contents_page_DIB(pdf, 6, 12)\n",
    "save_to_csv(flattened_contents, \"data_intensive\", 6, 12)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/ethics_2.pdf\")\n",
    "flattened_contents = find_contents_page_E1(pdf, 8, 14)\n",
    "save_to_csv(flattened_contents, \"ethics_2\", 8, 14)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/ethics_3.pdf\")\n",
    "flattened_contents = find_contents_page_E2(pdf, 6, 28)\n",
    "save_to_csv(flattened_contents, \"ethics_3\", 6, 28)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/os.pdf\")\n",
    "flattened_contents = find_contents_page_OS(pdf)\n",
    "save_to_csv(flattened_contents, \"os\",1,2)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/HCI.pdf\")\n",
    "flattened_contents = find_contents_page_HCI(pdf, 5, 10)\n",
    "save_to_csv(flattened_contents, \"HCI\", 5, 10)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/JavaScript.pdf\")\n",
    "flattened_contents = find_contents_page_JS(pdf, 4, 14)\n",
    "save_to_csv(flattened_contents, \"JavaScript\", 4, 14)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/network_1.pdf\")\n",
    "flattened_contents = find_contents_page_N1(pdf, 4, 11)\n",
    "save_to_csv(flattened_contents, \"network_1\", 4, 11)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/Robot_OS.pdf\")\n",
    "flattened_contents = find_contents_page_RO(pdf, 4, 9)\n",
    "save_to_csv(flattened_contents, \"Robot_OS\", 4, 9)\n",
    "\n",
    "pdf = PyPDF2.PdfReader(\"Books/Robotics_python.pdf\")\n",
    "flattened_contents = find_contents_page_RP(pdf, 4, 9)\n",
    "save_to_csv(flattened_contents, \"Robotics_python\", 4, 9)"
   ],
   "id": "9d9ea966a3242f3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: results/computer_architecture.csv\n",
      "CSV file created at: results/data_intensive.csv\n",
      "CSV file created at: results/ethics_2.csv\n",
      "CSV file created at: results/ethics_3.csv\n",
      "CSV file created at: results/os.csv\n",
      "CSV file created at: results/HCI.csv\n",
      "CSV file created at: results/JavaScript.csv\n",
      "CSV file created at: results/network_1.csv\n",
      "CSV file created at: results/Robot_OS.csv\n",
      "CSV file created at: results/Robotics_python.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T12:54:18.913281Z",
     "start_time": "2024-05-22T12:54:17.977773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pdf = PyPDF2.PdfReader(\"Books/ethics_2.pdf\")\n",
    "flattened_contents = find_contents_page_E2(pdf, 6, 28)\n",
    "save_to_csv(flattened_contents, \"ethics_2\", 6, 28)"
   ],
   "id": "cd43f33e13ec1e9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at: results/ethics_2.csv\n"
     ]
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
